{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b6ea54",
   "metadata": {},
   "source": [
    "# Stable Diffusion 1.5 + SAE\n",
    "\n",
    "⚠️WORK IN PROGRESS⚠️ - currently not ready for use\n",
    "\n",
    "This notebook demonstrates the use of Stable Diffusion 1.5 for concept unlearning using Sparse Autoencoders (SAE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb68713",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "This section handles imports, device configuration, and environment checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4673798c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# SAE part\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from einops import rearrange\n",
    "from IPython.display import display\n",
    "from overcomplete.visualization import overlay_top_heatmaps\n",
    "\n",
    "from utils.sae_utils import (\n",
    "    callback_handler,\n",
    "    criterion,\n",
    "    sae_integration_hook,\n",
    "    sae_train,\n",
    "    select_features,\n",
    ")\n",
    "\n",
    "model_id = \"sd-legacy/stable-diffusion-v1-5\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA device: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}\")\n",
    "print(f\"PyTorch CUDA version: {torch.version.cuda}\")\n",
    "print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df36759",
   "metadata": {},
   "source": [
    "## Model loading and activation caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81d2905",
   "metadata": {},
   "source": [
    "Model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ed31a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48311ab2",
   "metadata": {},
   "source": [
    "Add hooks in the Cross-Attention layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a466ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_activations = {}\n",
    "hook_handles = []\n",
    "\n",
    "# 1. Select Text Representation (Text Embedding)\n",
    "# The output of the last layer of the Text Encoder (always has shape: [1, 77, 768])\n",
    "text_encoder_output_layer = pipe.text_encoder.text_model.encoder.layers[-1]\n",
    "# hook_handles.append(\n",
    "#     text_encoder_output_layer.register_forward_hook(\n",
    "#         save_activation(\"text_embedding\")\n",
    "#     )\n",
    "# )\n",
    "\n",
    "# 2. Select Latent Representation (Latent Codes)\n",
    "# We will use the output of the second Transformer in Down Block 2\n",
    "target_unet_block_path = pipe.unet.up_blocks[1].attentions[1].transformer_blocks[0]\n",
    "# hook_handles.append(\n",
    "#     target_unet_block_path.register_forward_hook(\n",
    "#         save_activation(\"unet_latent_up_block_1_att_1\")\n",
    "#     )\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fce176",
   "metadata": {},
   "source": [
    "Model inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f330604b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inference_steps = 10\n",
    "guidance_scale = 8.0\n",
    "num_images_per_prompt = 1\n",
    "prompt = \"a football ball\"\n",
    "generator = torch.Generator(device).manual_seed(42)\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback_handler,\n",
    "    callback_steps=1,\n",
    "    num_images_per_prompt=num_images_per_prompt\n",
    "    ).images[0]\n",
    "\n",
    "display(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3378c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "cached_activations['unet_latent_up_block_1_att_1'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac463b2a",
   "metadata": {},
   "source": [
    "Zapis aktywacji w postaci tensora do pliku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aeed949",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing cached activations for SAE training\n",
    "guided_cached_activations = {}\n",
    "for i in range(0, cached_activations['unet_latent_up_block_1_att_1'].shape[0], 2):\n",
    "    guided_cached_activations[i//2] = cached_activations['unet_latent_up_block_1_att_1'][i:i+1]\n",
    "\n",
    "# Change to a tensor of shape (num_steps, batch_size=1, channels, height, width)\n",
    "guided_cached_activations_tensor = torch.cat(list(guided_cached_activations.values()), dim=0)\n",
    "# Save guided_cached_activations_tensor to file for later use\n",
    "torch.save(guided_cached_activations_tensor, \"guided_cached_activations_tensor.pt\")\n",
    "\n",
    "# Load guided_cached_activations_tensor from file\n",
    "guided_cached_activations_tensor = torch.load(\"guided_cached_activations_tensor.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5315652",
   "metadata": {},
   "source": [
    "## SAE trining example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cd1961",
   "metadata": {},
   "source": [
    "1. Get activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4a2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = torch.load(\"guided_cached_activations_tensor.pt\")\n",
    "activations = activations.float()\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2011012a",
   "metadata": {},
   "source": [
    "2. Transform into "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dac4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = rearrange(activations, 'n t d -> (n t) d').float()\n",
    "activations.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fa8f7",
   "metadata": {},
   "source": [
    "3. Use SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdc771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sae = sae_train(\n",
    "    activations,\n",
    "    criterion=criterion,\n",
    "    expansion_factor=16,\n",
    "    top_k=32,\n",
    "    batch_size=1024,\n",
    "    num_epochs=15,\n",
    "    learning_rate=1e-3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95d3553",
   "metadata": {},
   "source": [
    "## SEA feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd36786",
   "metadata": {},
   "outputs": [],
   "source": [
    "guided_cached_activations = torch.load(\"guided_cached_activations.pt\")\n",
    "\n",
    "guided_cached_activations_tensor_true = torch.cat(list(guided_cached_activations.values()), dim=0)\n",
    "\n",
    "guided_cached_activations_tensor_false = torch.load(\"guided_cached_activations_tensor.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7c7737",
   "metadata": {},
   "outputs": [],
   "source": [
    "select_features(\n",
    "    guided_cached_activations_tensor_true,\n",
    "    guided_cached_activations_tensor_false,\n",
    "    10,\n",
    "    sae\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b70943e",
   "metadata": {},
   "source": [
    "## Inference with SAE integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fcb62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the hook after training SAE\n",
    "hook_handles = []\n",
    "target_unet_block = pipe.unet.up_blocks[1].attentions[1].transformer_blocks[0]\n",
    "hook_handles.append(target_unet_block.register_forward_hook(sae_integration_hook))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709d08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear previous encodings\n",
    "num_inference_steps = 10\n",
    "guidance_scale = 8.0\n",
    "num_images_per_prompt = 1\n",
    "prompt = \"a football ball\"\n",
    "generator = torch.Generator(device).manual_seed(42)\n",
    "\n",
    "image = pipe(\n",
    "    prompt,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=guidance_scale,\n",
    "    generator=generator,\n",
    "    callback_on_step_end=callback_handler,\n",
    "    callback_steps=1,  # Wywołaj callback po każdym kroku\n",
    "    num_images_per_prompt=num_images_per_prompt\n",
    ").images[0]\n",
    "\n",
    "display(image)\n",
    "\n",
    "# After generation, inspect extracted encodings\n",
    "# print(f\"Extracted {len(sae_encodings)} sets of encodings (one per step).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b967417b",
   "metadata": {},
   "source": [
    "## Visualization of concept extraction\n",
    "requires cached_activations,sae,image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3079eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.float16,\n",
    "    safety_checker=None\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f5dc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = cached_activations['unet_latent_up_block_1_att_1'].to(device)\n",
    "with torch.no_grad():\n",
    "    pre_codes, codes = sae.encode(activations.reshape(-1, activations.shape[-1]))\n",
    "\n",
    "codes = rearrange(codes, '(b t) d -> b t d', b=activations.shape[0], t=activations.shape[1])\n",
    "codes = rearrange(codes, 'b (w h) d -> b w h d', w=16, h=16)\n",
    "\n",
    "image_tensor = np.array(image)\n",
    "image_tensor = torch.tensor(image_tensor).permute(2, 0, 1).unsqueeze(0).float().to(device) / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c421af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [39, 41, 78, 109, 154, 328, 362, 413, 513, 708]:\n",
    "  print('Concept', i)\n",
    "  # image_tensor: tensor shape (B, C, H, W), codes: tensor shape (B, C, 16, D)\n",
    "  overlay_top_heatmaps(image_tensor, codes, concept_id=i)\n",
    "  plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
